{"posts":[{"title":"[Survey][2020]Zero-Shot Learning","content":"Overview 监督学习的缺陷： 通过监督学习得到的学习器只能对训练数据覆盖的类的实例进行分类 缺乏对未知类的处理能力 可能出现未知类的原因： 目标类数量很大： 如CV方向中的对象识别。通常，人类可以识别的对象类别超过30000个，但为如此多的类分别收集足够多的标记实例是几乎不可能的。现有的数据集只能覆盖这些类中的一小部分，很多对象类没有标签实例。 目标类很稀有： 如细粒度的对象分类。如果需要识别不同品种的花朵种类，很难为每个特定的花卉品种收集足够的图像实例。对于很多稀有品种，我们找不到对应的带标签的实例。 目标类随时间变化： 如我们要识别某个品牌的产品实例。随着新样式、新产品的推陈出新，对于某些新产品，很难找到对应的带标签实例用于训练。 零样本学习： 目的：对属于未知类的实例进行分类处理 定义： 已知类集合：S={c_{i}^{s}}|i=1,\\dots,N_{s}}，其中cisc_{i}^{s}cis​是一个已知类 未知类集合：U=ciu∣i=1,…,NuU={c_{i}^{u}|i=1,\\dots,N_{u}}U=ciu​∣i=1,…,Nu​，其中ciuc_{i}^{u}ciu​是一个未知类 特征空间：XXX，通常为DDD维实数空间RDR^{D}RD 已知类中被标注的训练实例集合：Dtr=(xitr,yitr)∈X×Si=1NtrD^{tr}={(x_{i}^{tr},y_{i}^{tr})\\in X\\times S}_{i=1}^{N_{tr}}Dtr=(xitr​,yitr​)∈X×Si=1Ntr​​ 测试实例集合：Xte=xite∈Xi=1NteX^{te}={x_{i}^{te}\\in X}_{i=1}^{N_{te}}Xte=xite​∈Xi=1Nte​​ 测试实例对应类集合：Yte=yite∈Ui=1NteY^{te}={y_{i}^{te}\\in U}_{i=1}^{N_{te}}Yte=yite​∈Ui=1Nte​​ Zero-Shot Learning: 给予Dtr∈SD^{tr}\\in SDtr∈S，目的是为了学习得到能预测测试实例属于哪个未知类的分类器fu(⋅):X→Uf^{u}(\\cdot):X\\rightarrow Ufu(⋅):X→U 与少样本学习的异同： 零样本学习的一般思路通常是将训练实例DtrD^{tr}Dtr中包含的知识迁移到测试实例的分类任务中，而训练样本空间和测试样本空间是不相交的，因此，零样本学习其实是迁移学习的一个子领域。 在存在少量目标空间的标记实例的情况下，我们可以利用基于异构迁移学习的方法来尝试解决，但是在零样本学习中，因为根本没有属于目标空间（未知类）的标记实例，这使得现有的异构迁移学习方法表现不佳。 语义空间 在处理零样本学习问题时，由于没有属于未知类的标记实例可用，因此我们需要一些辅助信息。这类辅助信息应该包含有有关所有未知类的信息，以确保为每一个未知类提供相应的辅助信息；同时，辅助信息还应与特征空间中的实例相关，以确保辅助信息的可用性。 在现有作品中，涉及辅助信息的方法是受人类认识世界的方式启发的——我们可以借助语义背景知识来进行零样本学习。比如，只要知道“斑马看起来像马，而且它身上有条纹”，即使我们之前没有看到过斑马，我们也可以识别它。通常，零样本学习所涉及的辅助信息是一些语义信息，它们共同形成了一个既包含已知类，也包含未知类的空间。我们将这个空间成为语义空间。 与特征空间类似，语义空间一般也是实数空间，且每一个类都对应一个向量表示，称之为原型（prototype）。 定义： 语义空间：TTT，通常为一个MMM维的实数空间RMR^{M}RM 已知类的原型表示：ts∈Tt^{s}\\in Tts∈T表示csc^{s}cs对应的原型，其集合为Ts=tisi=1NsT^{s}={t^{s}_{i}}_{i=1}^{N_s}Ts=tis​i=1Ns​​ 未知类的原型表示：tu∈Tt^{u}\\in Ttu∈T表示cuc^{u}cu对应的原型，其集合为Tu=tiui=1NuT^{u}={t^{u}_{i}}_{i=1}^{N_u}Tu=tiu​i=1Nu​​ 原型函数：π(⋅):S⋃U→T\\pi(\\cdot):S\\bigcup U \\rightarrow Tπ(⋅):S⋃U→T 在零样本学习中，类原型TsT^{s}Ts和TuT^{u}Tu也会参与到分类器fu(⋅)f^{u}(\\cdot)fu(⋅)的获取过程中。 现有的模型中已经利用了各种语义空间，根据语义空间的构造方式，可以分为： 人工干预式语义空间 在这一类语义空间中，空间的每个维度都是为人所设计的。 属性空间 属性空间是由一组属性构成的语义空间。它们是零样本学习中使用最广泛的语义空间之一。 在属性空间中，描述类的各种属性的术语被定义为空间的属性。空间属性通常是与类属性相对应的短语或单词（如识别任务中的“毛色”、“栖息环境”等）。而这些空间属性将用于形成语义空间，每个维度都对应一个空间属性。 在动物识别任务中，考虑三个属性，“条纹”、“陆生”和“植食”三个属性。在这个空间中，“老虎类“对应的原型为[1,1,0][1,1,0][1,1,0]，”马类“对应的原型为[0,1,1][0,1,1][0,1,1]。 上述举例所得到的属性值都是二进制的，通常而言，这个属性值也可以是实数，表示程度或该类拥有这个属性的置信度。 词汇空间 词汇空间是一种由词汇信息构建的语义空间，它是基于已知类的类标签和能够提供语义信息的数据集的语义空间。这种数据集可以是一些结构化的词汇数据库，例如WordNet。 WordNet是普林斯顿大学开发的英语语料库，可以理解为就是一个词典。 最基本的，WordNet通过网状结构来组织词汇，将含义相近的词汇划分到一个组中。在这个网状结构之中，词汇与词汇之间的主要通过同义词连接在一起而形成了含义基本一致的group，称为synsets，也就是同义词形成的集合不同的synset之间的连接是通过conceptual relation连接到一起的。conceptual relation实际上包含了很多种关系 1.不同的synset通过上位词和下位词关系连接到一起。比如“树”可以和它的下位词“柳树”连接到一起，“柳树”可以连接它的下位词“垂柳”等，还可以是部分和整体的关系； 2.动词之间可以通过某方面的层层递进连接到一起，比如communicate-talk-whisper，通过音量的大小顺序连接，move-jog-run通过移动的快慢顺序连接； 3.动词之间也可以通过相互关联的动作连接在一起，比如buy-pay、success-try，虽然不是同义词，但是会经常同时发生，因此连接在一起，这里就会包含了因果关系，蕴含关系等； 4.形容词之间会将反义词进行连接，如wet-dry、young-old等，同时也会和它含义相似，但又不完全同义的词汇连接在一起； 5.副词大多数的含义和它相应的形容词含义相同； 6.词根相同的不同形态词之间会被连接到一起，如observe(verb)和observant(adjective)、observation、observatory(nouns)，在名词和动词构成的词对儿中，我们已经能够获得该名词相对于动词的具体含义了，比如sleeping_car是sleep的LOCATION 利用WordNet作为信息源，就能够构建多种结构的语义空间，如词汇的上下位关系、词汇之间的相似性、因果关系等。一般，我们可以计算WordNet中两个类cic_ici​和cjc_jcj​的距离关系来表示原型tit_iti​的第jjj个维度的值。 关键词空间 关键词空间是一种由每个类的文本描述中提取出的一组关键词所构成的语义空间。在关键词空间中，文本描述最常见的来源是网站文本，如Wikipedia等百科型网站和植物数据库等特定领域的专业性描述网站。除了预定义的网站外，这些文本内容还可以通过搜索引擎获得，通过对每个类名称的Google查询，获得描述该类的网页。 机器学习的语义空间 在这一类语义空间中，空间的维度不是人为设计的，每个类的原型是从某些机器学习模型的输出中获得的。在这些原型中，每个维度都没有明确的语义含义，相反，语义信息包含在整个原型中。用于提取原型的模型可以在其他问题中进行预训练，或者专门针对零样本学习问题进行训练。 标签嵌入空间 标签嵌入空间是一种通过对类标签做嵌入获得类原型的语义空间。这种空间的技术源自自然语言处理的词嵌入技术。在零样本学习中，每个类的标签都是单词或短语，嵌入过程中，语义相似的单词或短语被嵌入为临近的向量，而语义差距大的单词或短语则被嵌入为相距很远的向量。以此方法获得的嵌入向量便是语义空间的原型。 在现有作品中，采用了不同的嵌入技术，如：Word2Vec和Glove。此外，学习嵌入模型的语料库也包含一般的语料库（如Wikipedia）和特定的专业语料库（如Flickr）。 描述嵌入空间 描述嵌入空间是通过对类的文本描述做嵌入来获得类原型的语义空间。与人工干预式语义空间中的关键词空间类似，描述嵌入空间中的语义信息也来自于文本描述。 这两种方式的主要差异为：关键词空间通过提取文本描述中的关键词，并将每一个关键词作为语义空间中的一个维度来构建空间；而描述嵌入空间则通过一些学习模型来构建语义空间。这些模型的输入输出分别为文本描述和类原型。 在图像识别任务中，我们可以为每一类的图都收集一些文本描述。通过文本嵌入模型，我们可以将这些文本描述转化为对应的类原型，用于辅助识别可能出现的未知类。 优劣分析 人工干预式语义空间的优点是可以通过人为构造语义空间和类原型来灵活地利用人类的知识。缺点是该方法对人的依赖性过大，需要付出巨大的成本来完成语义空间的设计与构造； 机器学习的语义空间的优势则在于它们的生成过程相对省力，且生成的语义空间中包含有容易被人忽视的信息。缺点在于这类方法生成的空间，其每个维度的语义对人类而言是难以理解的，是隐性的。 零样本学习方法 现有的零样本学习方法可以分为两类，一类是基于分类器的方法，一类是基于实例的方法。 ​ 基于分类器的方法的重点在于如何直接为未知类学习分类器； ​ 基于实例的方法的重心则在于如何获取属于未知类的标记实例，并利用这些实例学习对应的分类器。 基于分类器的方法 根据构造分类器的方法，基于分类器的方法可以被进一步分为三个子类： 匹配方法 关系方法 组合方法 ​ 现有的基于分类器的方法通常采用一对多（one-versus-rest）方法类学习多类别零样本学习，即，对于每个未知类，都将学习一个二分类的一对多分类器。 匹配方法 这种方法的目的是通过建立类原型与该类对应的一对多分类器的关系来构建未知类的分类器。 在语义空间中，每个类都有且仅有一个对应的原型。因此，该原型可以被是为该类的一个“表示”；而在特征空间中，每个类都有一个对应的一对多分类器，该分类器也可以被视为该类的一个“表示”。匹配方法的目的是将这两种表示相匹配，通过输入的类原型和对应的一对多分类器学习获得匹配函数ϕ\\phiϕ，此后，根据获得的匹配函数和未知类的原型，便可以获得未知类的分类器。 A simple exponential family framework for zero-shot learning. 关系方法 利用可用数据，我们可以学习到已知类的分类器；此后，通过计算每个类对应的原型之间的关系，可以得到类之间的关系，利用这些关系和已知类的分类器，我们便可以为看不见的类构建分类器，并实现测试样本的分类。 分类器的学习与在监督学习中的学习类似，可以用SVM、神经网络的方法学习。而关系的选择则比较个性化：可以使用语义空间中原型之间的余弦相似度（计算两个向量之间的余弦值来表示相似度）；也可以利用WordNet中的本体结构或搜索引擎结果中的点击数量等。对于每个未知类，我们还可以根据不同的策略选择K个已知类来预测最有可能的K个分类结果。 Pooling objects for recognizing scenes without examples. 组合方法 在这种方法中，我们假设所有的类都是由一类更基本的成分组成的（类似于不同种类的夸克组合成质子和中子）。在语义空间中体现为空间的每个维度代表一种成分，每个类的原型代表了对应的类是由哪些成分组成。因此，考虑语义空间的结构，这种方法主要应用于one-hot编码的属性空间，1表示该类具有该成分/属性，0表示不具有。 在训练阶段，根据训练样本和已知类的原型我们可以训练出每一个空间属性对应的分类器fff。在测试阶段，我们可以根据这些分类器和特定的组合方法，利用未知类的原型进行分类。 组合方法举例：p(ciu∣xjte)=p(ciu)p(aiu)∏m=1Mp(ai,(m)u∣xjte)p(c^u_i|x^{te}_j)=\\frac{p(c^u_i)}{p(a^u_i)}\\prod_{m=1}^{M}p(a^u_{i,(m)}|x^{te}_j)p(ciu​∣xjte​)=p(aiu​)p(ciu​)​∏m=1M​p(ai,(m)u​∣xjte​)，其中p(ciu)p(c^u_i)p(ciu​)是未知类的先验概率，p(aiu)p(a^u_i)p(aiu​)为原型的先验概率，可以用训练结果求平均值表示，p(ai,(m)u∣xjte)p(a^u_{i,(m)}|x^{te}_j)p(ai,(m)u​∣xjte​)则表示测试样本XXX对应的原型是否拥有aaa属性。其目的是通过最大化后验概率推断未知类。 Learning to detect unseen object classes by between-class attribute transfer. 基于实例的方法 这种方法首先为未知类获取标记实例，然后使用这些实例来学习零样本分类器。根据这些实例的来源，现有的方法可以被分为三个子类： 投影方法 借用方法 投影方法 投影方法的主要方法是将特征空间的样本实例和语义空间的原型都投影到一个公共空间中来获得未知类的标记样本。 在特征空间中，有训练用的已知类的标记样本。同时，在语义空间中，存在已知和未知的类的原型。因为这两个空间都是实数空间，样本和原型都以向量形式存在，因此，原型也可以被视为特殊的标记向量。我们将投影的公共空间成为投影空间。 在投影方法中，每个未知类在特征空间中都没有标记样本，因此，它们在语义空间中的原型向量是它们的唯一标记样本。考虑到SVM、逻辑回归等分类器无法在这种样本数很少的情况下有效学习，我们仅考虑最近邻分类，即1NN：将测试样本投影到投影空间，然后利用1NN预测其类别。 投影空间可以是特征空间，可以是语义空间，也可以是另一个空间。 语义空间：Improving zero-shot learning by mitigating the hubness problem. 特征空间：Learning a deep embedding model for zero-shot learning. 其他空间：Zero-shot action recognition with error-correcting output codes. 借用方法 借用方法是通过从训练样本中借用来获得未知类的标记样本的方法。该方法基于类之间的相似性。比如，如果我们想要学习“卡车“类的分类器，在没有相应的标记样本的情况下，我们可以借用一些”汽车”和“公交车”的标记样本，因为它们与“卡车”相似。 在借用方法中，我们首先应确定未知类，进而知道需要借用的类是哪些类。在对训练样本的选取中，我们大致可以分为以下两种： 选择已知类的样本时会选择所有的已知类。对于一个未知类，可以借用所有已知类的样本，并根据原型计算出对应的相似度作为权重，利用这些加权的借用样本学习SVM分类器。 Cross-domain activity recognition. 仅选择某些特定的已知类样本。考虑到来自于相似度高的样本训练得到的模型会比相似度低的样本得到的模型优秀，所以对于每个未知类，都选择借用来自于高相似度的已知类的训练样本（基于语义空间的原型计算）。 Recognizing an action using its name: A knowledge-based approach. 讨论与发散 优劣分析 方法 优点 缺点 匹配方法 分类器与原型之间通过一个匹配函数来对应，简单有效 没有明确不同的类之间的关系 关系方法 类之间的关系被建模了，且在某些方法中，部分已知类分类器可以直接利用别的问题的结果 语义空间中类的关系被直接转移到了特征空间，从语义空间到特征空间的适应问题需要注意 组合方法 与关系方法类似，可以借用其他问题中学习到的属性分类器，减少模型学习成本 属性分类器的学习以及从属性到类的推断这两个步骤难以在一个优化过程中进行优化 投影方法 投影方式的选择非常灵活。投影函数的选择很多样化 每个未知类只有一个标记样本，因此只能使用最近邻分类 借用方法 未知类借用来的标记样本数量会相对较大，可以用监督学习中的方法来学习分类器 借用的样本实际上是属于已知类的样本，借用过多、过少都会导致准确率下降 多标签零样本学习 以上部分都是单标签零样本学习的方法，对于多标签零样本学习，通常会单独处理每个类标签，或者利用多标签之间的相关性。 Label embedding for zero-shot fine-grained named entity typing. Costa: Co-occurrence statistics for zero-shot classification. A model of zero-shot learning of spoken language understanding. Multi-label zero-shot learning with structured knowledge graphs. Predicting unseen labels using label hierarchies in large-scale multi-label learning. Multiple instance visual-semantic embedding. Fast zero-shot image tagging. ","link":"https://Yhaoze.github.io/post/summary2020zero-shot-learning-settings-methods-and-applications/"},{"title":"[Paper][NAACL-2019]HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition","content":"Overview 应用场景：对话模型中的情感识别 解决问题： 相同的单词在不同的上下文语境中可以传递不同的情感； 在一般对话中很少见到某些强烈的情绪情感； 很难有效地获取远程上下文信息。 主要贡献： 提出了HiGRU框架，能够更准确地学习语句的嵌入信息，并据此识别情感； 提出了两种渐进式的HiGRU变体——HiGRU-f和HiGRU-sf，分别融合各个单词、话语级别的信息和长时上下文信息。 数据来源： IEMOCAP：它包含了大约12个小时的视听数据，包括视频、语音、面部动作捕捉和文本转录内容，含有四种情感标签。 FRIENDS：来自“老友记”的对话（及标注），包含八种情感标签（但进选取四个训练）。 EmotionPush: 这个数据集中包含了1000个Facebook上的私人对话，包含有和FRIENDS相同的八种情感标签（同样只选取四个训练）。 Model HiGRU（上图出去两个虚线框后的模型结构）: 主体结构分为两层，网络下层为左半边模型，在得到文本、话语的词嵌入（字嵌入）后，通过BiGRU网络（激励函数为tanh函数）获取句嵌入；网络上层为右半边模型，在得到句嵌入后，通过相似的结构（BiGRU）获得基于上下文信息的句嵌入，此后，一次通过全连接层和Softmax函数得到最终的情感分类。 HiGRU-f（HiGRU+Fusion层）：在HiGRU结构的基础上又添加了Fusion层。Fusion层的作用是将各输入（下层网络中的词嵌入和上层网络中的句嵌入）与BiGRU结构的隐藏状态（hforwardh_{forward}hforward​和hbackwardh_{backward}hbackward​）输出连接起来作为BiGRU结构的整体隐藏层。 HiGRU-sf（HiGRU+Fusion+Attention）：在HiGRU结构的基础上又添加了Fusion层和Attention层，Fusion层同HiGRU-f，Attention层为自注意力层（self-attention），示意图如下： 其中： Experiment 验证了在IEMOCAP数据集上，HiGRU及两个变体都取得了更好的情感分类效果： 验证了在FRIENDS和EmotionPush数据集上能得到更好的效果，且训练集不同，结果各有差异： （F代表只在FRIENDS数据集上训练，E代表只在EmotionPush数据集上训练，F+E代表两者共同作为训练集） ","link":"https://Yhaoze.github.io/post/papernaacl-2019higru-hierarchical-gated-recurrent-units-for-utterance-level-emotion-recognition/"},{"title":"[Paper][NAACL-2019]Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis","content":"Overview 应用场景：多种不同的信息输入（如：文本、音频、视频信息）情况下的情感识别 主要贡献： 利用有效的多模式框架，将两个相关任务（即情感识别和情绪分类任务）的相互依存关系改善结果； 提出了语境间（inter-modal）注意力机制，该机制有助于模型同时为做出贡献的语境话语和/或语气分配权重； 提供了“最先进的”（文章自称）情感识别和情绪分类方法。 数据来源：CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset ​ 这个数据集包含了文本、音频、视频三种不同模式的输入。 Model embedding: Text - GloVe; Visual - Facets; Acoustic - CovaRap. 主模型：三种不同模式的输入信息分别通过三个BiGRU网络得到三个不同的特征矩阵，接着由CIM-Attention层学习模式两两之间的关联情况，最后分别通过softmax层（情感识别）和sigmoid层（情绪分类）得到最终的结果。 CIM-Attention: Experiment 验证了多任务模型会比单任务模型具有更好的效果： 对CIM-Attention的attention weights做了可视化处理，帮助解释模型的工作原理： ","link":"https://Yhaoze.github.io/post/papernaacl-2019multi-task-learning-for-multi-modal-emotion-recognition-and-sentiment-analysis/"},{"title":"[Paper][EMNLP-2019]Multi-label Categorization of Accounts of Sexism using a Neural Framework","content":"Overview 应用场景：网络上关于性别歧视内容的识别与分类 数据来源：Everyday Sexism Project website中的13023个条目，并由10名注释人员标注标签 嵌入模型： 词嵌入： ELMo (Embeddings from Language Models) GloVe (Global Vectors) fastText 句嵌入： BERT (Bidirectional Encoder Representations from Transformers) USE (Universal Sentence Encoder) InferSent 贡献： 提出了一个用于性别歧视问题的、多标签分类的神经框架 对任何类型的性别歧视问题进行了考虑 提供了一个包括13023个性别歧视说明的数据集，并制定了23种性别歧视类别 Model word embedding部分为多个不同的词嵌入模块共同组成； sentence embedding部分为多个不同的句嵌入模块组成； configurable word-level concatenated部分是通过不同的词嵌入模块得到的嵌入向量组合得到的不同结果； configurable sentence-level concatenated部分是通过不同句嵌入模块得到的嵌入向量和通过CNN/biLSTM学习得到的词嵌入的特征向量组合得到的不同结果。 loss函数部分：由于交叉熵函数不适合多标签的分类任务，提供了两个变种。 EBCE: EBCE weight: NCE: NCE weight: Experiment 验证了传统的机器学习算法在多标签任务中的短板： 检验了不同的模型结构（CNN/biLSTM; word embedding/sentence embedding; EBCE loss/NCE loss）对结果的影响: ","link":"https://Yhaoze.github.io/post/paperemnlp-2019multi-label-categorization-of-accounts-of-sexism-using-a-neural-framework/"}]}